% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/somobj_doc.R
\name{train_SOM}
\alias{train_SOM}
\title{Training function for a SOM object}
\usage{
SOMobj$train_SOM(nsteps, X)
}
\arguments{
\item{nsteps}{the number of training steps to perform}

\item{X}{the training data matrix. 
This should be the same matrix that was input to \link{initialize_SOM} 
(checks will be performed on its statistics, if they do not match an error will be returned).}
}
\value{
None, SOM products are updated internally
}
\description{
(C)SOM training is performed and all associated learning products updated internally, 
according to DeSieno's algorithm, using the learning rates specified in \link{set_LRAS}.
}
\details{
This function calls many of the helper function of the SOM class internally 
(e.g., \link{update_p}, \link{update_W}, \link{map_to_netrng}, \link{update_learning_rates}). 
 
Training is performed online (not batch) according to a sequential random sampling of the rows of \code{X}. 
Reproducibility of training results can be achieved by calling R's \link[base]{set.seed} function prior to calling \code{train_SOM}, 
which will control the random seed used for sampling \code{X}.  The training order used is during each call to \code{train_SOM} is stored 
in the field \code{train_order}.  
 
Additionally, \link{recall_SOM} is called at the end of training to update the SOM products.  
 
Parallel computation for BMU selection and prototype updating can be achieved by calling \link{set_parallel} prior to calling \code{train_SOM}. 
 
Various visualizations exist to examine the results of SOM training. See any of the \code{vis_som_*} functions for more information.
}
\references{
\insertRef{DeSieno1988}{SOMDisco}
}
