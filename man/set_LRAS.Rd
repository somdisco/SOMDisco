% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/somobj_doc.R
\name{set_LRAS}
\alias{set_LRAS}
\title{Set the Learning Rate Annealing Schedule}
\usage{
SOMobj$set_LRAS(LRAS)
}
\arguments{
\item{LRAS}{a data frame, which must have the above structure.}
}
\value{
None, the LRAS is stored internally
}
\description{
The LRAS is a data frame with columns: 
\itemize{
\item t \item alpha \item beta \item gamma \item sigma
}
The rows of the data frame specify the learning parameters \code{alpha}, \code{beta}, \code{gamma} and \code{sigma} 
that are in effect up to (but not including) the training step identified in \code{t}.  
At each training step alpha controls the strength of the prototype update, beta controls the strength of the win frequency update, 
gamma controls the magnitude of the entropy-maximizing bias, and sigma controls the maximum radius of the neighborhood function eta. 

Internally, the columns of the input are extracted and stored in a \code{std::map} for quick lookup. At each training step, effective 
values from this map are extracted and stored in fields \code{alpha}, \code{beta}, \code{gamma} and \code{sigma} (the latter, in turn, 
also updates the neighborhood function \code{eta}). 

A default LRAS, based on the number of training sampled, can be obtained via \link{default_LRAS}.  

Internally, a final time step \code{t = std::max_element<unsigned int>} is appended to the LRAS, which has the effect of 
recycling the last set of learning parameters for indefinite training.  

Kohonen's original SOM algorithm can be achieved by setting both beta and gamma = 0 in the LRAS.
}
\references{
\insertRef{DeSieno1988}{SOMDisco}
}
