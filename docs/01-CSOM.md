Background
==========

Kohonen‚Äôs SOM
-------------

Kohonen‚Äôs Self-Organizing Map (SOM, \[@Kohonen2001\]) is an unsupervised
neural network for high-dimensional manifold learning and
low-dimensional representation of this learning. During training a
pre-defined number of **prototype** vectors
*W*‚ÄÑ=‚ÄÑ{*w*<sub>*j*</sub>‚ÄÑ‚àà‚ÄÑ‚Ñù<sup>*d*</sup>}<sub>*j*‚ÄÑ=‚ÄÑ1</sub><sup>*n*<sub>*W*</sub></sup>,
 are formed to best represent given training data
*X*‚ÄÑ=‚ÄÑ{*x*<sub>*s*</sub>‚ÄÑ‚àà‚ÄÑ‚Ñù<sup>*d*</sup>}<sub>*s*‚ÄÑ=‚ÄÑ1</sub><sup>*n*<sub>*X*</sub></sup>
 which are assumed to be sampled from an (unknown) manifold ‚Ñ≥. Each
prototype vector *w*<sub>*j*</sub> is associated with a **neuron**
*ŒΩ*<sub>*j*</sub>, which is an element of the SOM‚Äôs **output space** ‚Ñí
(usually a lattice topology in ‚Ñù<sup>2</sup> or possibly ‚Ñù<sup>3</sup>,
SOMDisco exclusively uses 2-d lattices). As the prototype vectors are
formed the neurons are simultaneously **organized** on ‚Ñí such that they
are **topology preserving** ‚Äì that is, neighboring prototypes in
‚Ñù<sup>*d*</sup> are associated with neurons neighboring neurons on ‚Ñí.
Salient features of ‚Ñ≥ are then faithfully represented on ‚Ñí, allowing an
analyst to infer high-dimensional data characteristics (such as, e.g.,
cluster structure) from their lattice representation.

<img src="./figs/som_anatomy.png" width="70%" style="display: block; margin: auto;" />

This organization is enforced by cooperative updates to the prototypes
during the learning process. At training step *t*, prototype updates are
governed by
*w*<sub>*j*</sub><sup>*t*</sup>‚ÄÑ=‚ÄÑ*w*<sub>*j*</sub><sup>*t*‚ÄÖ‚àí‚ÄÖ1</sup>‚ÄÖ+‚ÄÖ*Œ±*<sub>*t*</sub>‚ÄÖ√ó‚ÄÖ*Œ∑*(*j*,‚ÄÜ*j*<sub>*t*</sub><sup>\*</sup>‚ÄÜ|‚ÄÜ*œÉ*<sub>*t*</sub>)‚ÄÖ√ó‚ÄÖ(*x*<sup>*t*</sup>‚ÄÖ‚àí‚ÄÖ*w*<sub>*j*</sub>)
 where

-   *Œ±*<sub>*t*</sub> is a learning parameter controlling the amount by
    which prototypes are allowed to change at step *t*, which should be
    annealed over time
-   *x*<sup>*t*</sup>‚ÄÑ‚àà‚ÄÑ*X* is a training vector chosen at random for
    presentation to the network at time *t*, also called the network
    **stimulus** at time *t*
-   *Œ∑* (the **neighborhood function**) is a non-increasing function of
    the lattice distance between neurons *ŒΩ*<sub>*j*</sub> and
    *ŒΩ*<sub>*j*<sub>*t*</sub><sup>\*</sup></sub>, where
    *j*<sub>*t*</sub><sup>\*</sup> indexes the prototype closest to the
    datum *x*<sup>*t*</sup>. We call this the **Best Matching Unit** of
    *x*<sup>*t*</sup>:
    *B**M**U*(*x*<sup>*t*</sup>)‚ÄÑ:=‚ÄÑ*j*<sub>*t*</sub><sup>\*</sup>‚ÄÑ=‚ÄÑarg‚ÄÜmin<sub>*j*</sub>‚ÄÜ*d*<sub>*E*</sub>(*w*<sub>*j*</sub>,‚ÄÜ*x*<sup>*t*</sup>)
     where *d*<sub>*E*</sub> is standard Euclidean distance.  
-   *œÉ*<sub>*t*</sub> is a learning parameter controlling the size of a
    lattice neighborhood centered about neuron
    *ŒΩ*<sub>*j*<sub>*t*</sub><sup>\*</sup></sub>; neurons residing in
    this neighborhood are affected more strongly by the stimulus
    *x*<sup>*t*</sup>, with affect inversely proportional to their
    lattice distance to the BMU.

The implication of including the composition of several notions of
distance (i.e., in both ‚Ñù<sup>*d*</sup> and ‚Ñí) in the prototype update
rule forms a highly non-linear mapping *Œ¶*‚ÄÜ:‚ÄÜ‚Ñ≥‚ÄÑ‚Üí‚ÄÑ‚Ñí. *Œ¶* is
**self-organized**, meaning the relationship between neuron proimities
on ‚Ñí and prototype proximities in ‚Ñ≥ arises organically, without direct
optimization of any loss function. SOM Learning is modeled after the
organization and compartmentalization of the learning process which
occurs in the human brain.

Conscience SOM
--------------

Kohonen‚Äôs prototype updates can yield a mapping *Œ¶* which under-utilizes
the quantizer‚Äôs full codebook (i.e., the set of prototypes *W*). Such
under-utilizations degrade the richess of the representation of ‚Ñ≥ by ùí≤
(and on ‚Ñí). To correct this, DeSieno introduced the **Conscience SOM**
(or **CSOM**, \[@DeSieno1988\]), which introduces a **bias** to
Kohonen‚Äôs algorithm to achieve a mapping *Œ¶* with higher entropy. Each
prototype possesses its own bias *b*<sub>*j*</sub><sup>*t*</sup> which
influences the competitive stage of SOM learning such that CSOM BMU
selection at time *t* becomes
*B**M**U*(*x*<sup>*t*</sup>)‚ÄÑ=‚ÄÑarg‚ÄÜmin<sub>*j*</sub>‚ÄÜ*d*<sub>*E*</sub>(*w*<sub>*j*</sub>,‚ÄÜ*x*<sup>*t*</sup>)‚ÄÖ‚àí‚ÄÖ*b*<sub>*j*</sub><sup>*t*</sup>.
 The biases {*b*<sub>*j*</sub>} are formed alongside the prototype
vectors during learning via the use of auxiliary quantities
{*p*<sub>*j*</sub>} representing the historical **win frequencies** of
each prototype (i.e, proportion of times a prototype has quantized data
during learning). These win frequencies are updated during each learning
step by
*p*<sub>*j*</sub><sup>*t*</sup>‚ÄÑ=‚ÄÑ*p*<sub>*j*</sub><sup>*t*‚ÄÖ‚àí‚ÄÖ1</sup>‚ÄÖ+‚ÄÖ*Œ≤*<sub>*t*</sub>‚ÄÖ√ó‚ÄÖ(ùüô\[*j*‚ÄÑ=‚ÄÑ*j*<sub>*t*</sub><sup>\*</sup>\]‚ÄÖ‚àí‚ÄÖ*p*<sub>*j*</sub><sup>*t*‚ÄÖ‚àí‚ÄÖ1</sup>)
 yielding a bias
$$ b\_j^t = \\gamma\_t \\times \\left( \\frac{1}{n\_W} - p\_j^t \\right) $$
 For maximum-entropy mapping, *p*<sub>*j*</sub>‚ÄÑ=‚ÄÑ1/*n*<sub>*W*</sub>
implying zero bias. The additional learning parameters *Œ≤*<sub>*t*</sub>
and *Œ≥*<sub>*t*</sub> control the degree to which the SOM‚Äôs ‚ÄúConscience‚Äù
influences its learning. Like the learning rate *Œ±*<sub>*t*</sub> they
should both be annealed over time.

A properly trained CSOM can achieve more effective utilization of the
SOM‚Äôs output space \[@DeSieno1988\] which aides (cluster) inference from
a trained map. In contrast to Kohonen‚Äôs cooperative function (*Œ∑*),
DeSieno also suggested restricting prototype updates to a very small
neighborhood of neurons (e.g., *œÉ*<sub>*t*</sub>‚ÄÑ=‚ÄÑ1), which decreases
the computational burden of CSOM learning (as compared to Kohonen‚Äôs
SOM). Moreover, as all prototypes are encouraged to participate in the
learned representation of ‚Ñ≥ the resulting view of the manifold is often
richer, providing for more sophisticated cluster extraction tools such
as the CADJ matrix.

The CADJ Matrix
---------------

**CADJ** (or **C**umulative **ADJ**acency, \[@TasdemirMerenyi2009\]) is
a weighted adjacency matrix of SOM prototypes (or, equivalently, their
associated neurons). The edge weights of this **Topology Representing
Network** \[@MartinetzSchulten1994\] are given by
*C**A**D**J*<sub>*i**j*</sub>‚ÄÑ=‚ÄÑ\#{*x*<sub>*s*</sub>‚ÄÜ:‚ÄÜ*B**M**U*1(*x*<sub>*s*</sub>)‚ÄÑ=‚ÄÑ*i* and *B**M**U*2(*x*<sub>*s*</sub>)‚ÄÑ=‚ÄÑ*j*}
 where BMU1 and BMU2 are the first and second-ranking BMU, respectively.
Analysis of the CADJ values reveals the ‚Äúseams‚Äù (disconnects) of a
learned manifold, which are helpful for discerning cluster structure.

While CADJ can technically be computed using the codebook of *any*
vector quantizer as vertices, the SOM‚Äôs output space ‚Ñí provides prime
real estate on which to view and analyze this graph, regardless of data
dimension *d*. The **CONNvis** visiualization \[@TasdemirMerenyi2009\],
derived from the symmetrized **CONN**ectivity graph,
*C**O**N**N*‚ÄÑ=‚ÄÑ*C**A**D**J*‚ÄÖ+‚ÄÖ*C**A**D**J*<sup>*T*</sup>,
 has been used repeatedly to elicit cluster structure from complex,
high-dimensional manifolds with **no restriction** on cluster size
(number of data members), shape (e.g., Gaussianinity), or dimension
(low-rank embeddings). These settings are precisely where performance of
other clustering routines (e.g., hierarchical methods, k-means, mixture
models) deterioriate.

Despite the demonstrated benefits and sophistication of the combination
of CSOM manifold learning and subsequent CADJ-based cluster extraction
(CITE), neither of these tools are currently publicly available to the
machine learning community. SOMDisco rectifies this omission.
