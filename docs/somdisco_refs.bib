% mU-matrix
@ARTICLE{MerenyiJainVillmann,
  author={E. {Merényi} and A. {Jain} and T. {Villmann}},
  journal={IEEE Transactions on Neural Networks},
  title={Explicit Magnification Control of Self-Organizing Maps for “Forbidden” Data},
  year={2007},
  volume={18},
  number={3},
  pages={786-797},}


@article{Lowess,
author = { William S.   Cleveland },
title = {Robust Locally Weighted Regression and Smoothing Scatterplots},
journal = {Journal of the American Statistical Association},
volume = {74},
number = {368},
pages = {829-836},
year  = {1979},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.1979.10481038},

URL = {
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038

},
eprint = {
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1979.10481038

}
}

@article{Toeplitz,
 ISSN = {19326157},
 URL = {http://www.jstor.org/stable/23566492},
 abstract = {Simulating sample correlation matrices is important in many areas of statistics. Approaches such as generating Gaussian data and finding their sample correlation matrix or generating random uniform [-1, 1] deviates as pair-wise correlations both have drawbacks. We develop an algorithm for adding noise, in a highly controlled manner, to general correlation matrices. In many instances, our method yields results which are superior to those obtained by simply simulating Gaussian data. Moreover, we demonstrate how our general algorithm can be tailored to a number of different correlation models. Using our results with a few different applications, we show that simulating correlation matrices can help assess statistical methodology.},
 author = {Johanna Hardin and Stephan Ramon Garcia and David Golan},
 journal = {The Annals of Applied Statistics},
 number = {3},
 pages = {1733--1762},
 publisher = {Institute of Mathematical Statistics},
 title = {A METHOD FOR GENERATING REALISTIC CORRELATION MATRICES},
 volume = {7},
 year = {2013}
}


@inproceedings{DeSieno1988,
  title={Adding a conscience to competitive learning},
  author={DeSieno, Duane},
  booktitle={IEEE international conference on neural networks},
  volume={1},
  number={6},
  pages={117--124},
  year={1988},
  organization={IEEE Piscataway, NJ}
}


@inproceedings{UMatrix,
  added-at = {2016-01-13T15:31:26.000+0100},
  address = {Dordrecht, Netherlands},
  author = {Ultsch, Alfred and Siemon, H. Peter},
  biburl = {https://www.bibsonomy.org/bibtex/21c4380edab426b258522e53b84da6104/mhwombat},
  booktitle = {Proceedings of the International Neural Network
                 Conference (INNC-90), Paris, France, July 9–13, 1990
                 1. Dordrecht, Netherlands},
  editor = {Widrow, Bernard and Angeniol, Bernard},
  interhash = {e7afda41f19a29d5b2ad93869e84de20},
  intrahash = {1c4380edab426b258522e53b84da6104},
  keywords = {som},
  pages = {305--308},
  publisher = {Kluwer Academic Press},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {Kohonen's Self Organizing Feature Maps for Exploratory
                 Data Analysis},
  url = {http://www.uni-marburg.de/fb12/datenbionik/pdf/pubs/1990/UltschSiemon90},
  volume = 1,
  year = 1990
}


@article{TasdemirMerenyi2009,
author={K. Ta\c sdemir and E. Mer\' enyi},
journal={IEEE Transactions on Neural Networks},
title={Exploiting Data Topology in Visualization and Clustering of Self-Organizing Maps},
year={2009},
volume={20},
number={4},
pages={549-562},
abstract={The self-organizing map (SOM) is a powerful method for visualization, cluster extraction, and data mining. It has been used successfully for data of high dimensionality and complexity where traditional methods may often be insufficient. In order to analyze data structure and capture cluster boundaries from the SOM, one common approach is to represent the SOM's knowledge by visualization methods. Different aspects of the information learned by the SOM are presented by existing methods, but data topology, which is present in the SOM's knowledge, is greatly underutilized. We show in this paper that data topology can be integrated into the visualization of the SOM and thereby provide a more elaborate view of the cluster structure than existing schemes. We achieve this by introducing a weighted Delaunay triangulation (a connectivity matrix) and draping it over the SOM. This new visualization, CONNvis, also shows both forward and backward topology violations along with the severity of forward ones, which indicate the quality of the SOM learning and the data complexity. CONNvis greatly assists in detailed identification of cluster boundaries. We demonstrate the capabilities on synthetic data sets and on a real 8D remote sensing spectral image.},
keywords={data mining;data structures;data visualisation;knowledge representation;learning (artificial intelligence);mesh generation;pattern clustering;self-organising feature maps;SOM;cluster extraction;connectivity matrix;data complexity;data mining;data structure;data topology;data visualization;high dimensionality data;knowledge representation;machine learning;self-organizing map;weighted Delaunay triangulation;Clustering;data mining;self-organizing map (SOM);topology preservation;visualization},
doi={10.1109/TNN.2008.2005409},
ISSN={1045-9227},
month={April},}


@book{Kohonen2001,
  Author = {Teuvo Kohonen},
  Title = {Self-Organizing Maps},
  Publisher = {Springer},
  Year = {2000},
  ISBN = {3540679219}
}

@article{MartinetzSchulten1994,
title = "Topology representing networks",
journal = "Neural Networks",
volume = "7",
number = "3",
pages = "507 - 522",
year = "1994",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(94)90109-0",
url = "http://www.sciencedirect.com/science/article/pii/0893608094901090",
author = "Thomas Martinetz and Klaus Schulten",
keywords = "Proximity problems",
keywords = "Delaunay triangulation",
keywords = "Voronoi polyhedron",
keywords = "Hebb rule",
keywords = "Winner-take-all competition",
keywords = "Topology preserving feature map",
keywords = "Topology representation",
keywords = "Path preservation",
keywords = "Path planning",
abstract = "Abstract A Hebbian adaptation rule with winner-take-all like competition is introduced. It is shown that this competitive Hebbian rule forms so-called Delaunay triangulations, which play an important role in computational geometry for efficiently solving proximity problems. Given a set of neural units i, i = 1,…, N, the synaptic weights of which can be interpreted as pointers wi, i = 1,…, N in RD, the competitive Hebbian rule leads to a connectivity structure between the units i that corresponds to the Delaunay triangulation of the set of pointers wi. Such competitive Hebbian rule develops connections (Cij > 0) between neural units i, j with neighboring receptive fields (Voronoi polygons) Vi, Vj, whereas between all other units i, j no connections evolve (Cij = 0). Combined with a procedure that distributes the pointers wi over a given feature manifold M, for example, a submanifold M ⊂ RD, the competitive Hebbian rule provides a novel approach to the problem of constructing topology preserving feature maps and representing intricately structured manifolds. The competitive Hebbian rule connects only neural units, the receptive fields (Voronoi polygons) Vi, Vj of which are adjacent on the given manifold M. This leads to a connectivity structure that defines a perfectly topology preserving map and forms a discrete, path preserving representation of M, also in cases where M has an intricate topology. This makes this novel approach particularly useful in all applications where neighborhood relations have to be exploited or the shape and topology of submanifolds have to be take into account."
}


