[
["index.html", "SOMDisco Fast Learning and Visualization of Conscience-SOMs Preface", " SOMDisco Fast Learning and Visualization of Conscience-SOMs Josh Taylor Preface SOMDisco is an R package intended to address several advances in SOM learning and analysis that are currently missing in currently available SOM packages for R (som, kohonen, and popsom). The main contributions of SOMDisco are: Integration of DeSieno’s Conscience-SOM modifications to Kohonen’s original algorithm to attempt maximum entropy SOM learning Fast and efficient C++ implementation of CSOM training (based on Rcpp and RcppArmadillo) Optional parallel SOM training and recall (as applicable, via RcppParallel) Computation of the CADJ matrix, which is a weighted topological adjacency of SOM prototypes helpful in cluster discovery The CONNvis visualization (requires the TopoRNet R package) which represents the topological information in CADJ on the SOM lattice Other advanced SOM visualizations (the mU-matrix, propagation of discrete and continuous values on the lattice) "],
["background.html", "Chapter 1 Background 1.1 Kohonen’s SOM 1.2 Conscience SOM 1.3 The CADJ Matrix", " Chapter 1 Background 1.1 Kohonen’s SOM Kohonen’s Self-Organizing Map (SOM, [1]) is an unsupervised neural network for high-dimensional manifold learning and low-dimensional representation of this learning. During training a pre-defined number of prototype vectors \\[W = \\{w_j \\in \\mathbb{R}^d \\}_{j=1}^{n_W}, \\] are formed to best represent given training data \\[ X = \\{x_s \\in \\mathbb{R}^d \\}_{s=1}^{n_X} \\] which are assumed to be sampled from an (unknown) manifold \\(\\mathcal{M}\\). Each prototype vector \\(w_j\\) is associated with a neuron \\(\\nu_j\\), which is an element of the SOM’s output space \\(\\mathcal{L}\\) (usually a lattice topology in \\(\\mathbb{R}^2\\) or possibly \\(\\mathbb{R}^3\\), SOMDisco exclusively uses 2-d lattices). As the prototype vectors are formed the neurons are simultaneously organized on \\(\\mathcal{L}\\) such that they are topology preserving – that is, neighboring prototypes in \\(\\mathbb{R}^d\\) are associated with neurons neighboring neurons on \\(\\mathcal{L}\\). Salient features of \\(\\mathcal{M}\\) are then faithfully represented on \\(\\mathcal{L}\\), allowing an analyst to infer high-dimensional data characteristics (such as, e.g., cluster structure) from their lattice representation. This organization is enforced by cooperative updates to the prototypes during the learning process. At training step \\(t\\), prototype updates are governed by \\[ w_j^{t} = w_j^{t-1} + \\alpha_t \\times \\eta(j, j^*_t \\, | \\, \\sigma_t) \\times (x^t - w_j) \\] where \\(\\alpha_t\\) is a learning parameter controlling the amount by which prototypes are allowed to change at step \\(t\\), which should be annealed over time \\(x^t \\in X\\) is a training vector chosen at random for presentation to the network at time \\(t\\), also called the network stimulus at time \\(t\\) \\(\\eta\\) (the neighborhood function) is a non-increasing function of the lattice distance between neurons \\(\\nu_j\\) and \\(\\nu_{j^*_t}\\), where \\(j^*_t\\) indexes the prototype closest to the datum \\(x^t\\). We call this the Best Matching Unit of \\(x^t\\): \\[ BMU(x^t) := j^*_t = \\arg\\min_j \\, d_E(w_j, x^t) \\] where \\(d_E\\) is standard Euclidean distance. \\(\\sigma_t\\) is a learning parameter controlling the size of a lattice neighborhood centered about neuron \\(\\nu_{j^*_t}\\); neurons residing in this neighborhood are affected more strongly by the stimulus \\(x^t\\), with affect inversely proportional to their lattice distance to the BMU. The implication of including the composition of several notions of distance (i.e., in both \\(\\mathbb{R}^d\\) and \\(\\mathcal{L}\\)) in the prototype update rule forms a highly non-linear mapping \\(\\Phi \\, : \\, \\mathcal{M} \\to \\mathcal{L}\\). \\(\\Phi\\) is self-organized, meaning the relationship between neuron proimities on \\(\\mathcal{L}\\) and prototype proximities in \\(\\mathcal{M}\\) arises organically, without direct optimization of any loss function. SOM Learning is modeled after the organization and compartmentalization of the learning process which occurs in the human brain. 1.2 Conscience SOM Kohonen’s prototype updates can yield a mapping \\(\\Phi\\) which under-utilizes the quantizer’s full codebook (i.e., the set of prototypes \\(W\\)). Such under-utilizations degrade the richess of the representation of \\(\\mathcal{M}\\) by \\(\\mathcal{W}\\) (and on \\(\\mathcal{L}\\)). To correct this, DeSieno introduced the Conscience SOM (or CSOM, [2]), which introduces a bias to Kohonen’s algorithm to achieve a mapping \\(\\Phi\\) with higher entropy. Each prototype possesses its own bias \\(b_j^t\\) which influences the competitive stage of SOM learning such that CSOM BMU selection at time \\(t\\) becomes \\[ BMU(x^t) = \\arg\\min_j \\, d_E(w_j, x^t) - b_j^t.\\] The biases \\(\\{b_j\\}\\) are formed alongside the prototype vectors during learning via the use of auxiliary quantities \\(\\{p_j\\}\\) representing the historical win frequencies of each prototype (i.e, proportion of times a prototype has quantized data during learning). These win frequencies are updated during each learning step by \\[ p_j^t = p_j^{t-1} + \\beta_t \\times (\\mathbb{1}[j = j^*_t] - p_j^{t-1}) \\] yielding a bias \\[ b_j^t = \\gamma_t \\times \\left( \\frac{1}{n_W} - p_j^t \\right) \\] For maximum-entropy mapping, \\(p_j = 1 / n_W\\) implying zero bias. The additional learning parameters \\(\\beta_t\\) and \\(\\gamma_t\\) control the degree to which the SOM’s “Conscience” influences its learning. Like the learning rate \\(\\alpha_t\\) they should both be annealed over time. A properly trained CSOM can achieve more effective utilization of the SOM’s output space [2] which aides (cluster) inference from a trained map. In contrast to Kohonen’s cooperative function (\\(\\eta\\)), DeSieno also suggested restricting prototype updates to a very small neighborhood of neurons (e.g., \\(\\sigma_t = 1\\)), which decreases the computational burden of CSOM learning (as compared to Kohonen’s SOM). Moreover, as all prototypes are encouraged to participate in the learned representation of \\(\\mathcal{M}\\) the resulting view of the manifold is often richer, providing for more sophisticated cluster extraction tools such as the CADJ matrix. 1.3 The CADJ Matrix CADJ (or Cumulative ADJacency, [3]) is a weighted adjacency matrix of SOM prototypes (or, equivalently, their associated neurons). The edge weights of this Topology Representing Network [4] are given by \\[ CADJ_{ij} = \\# \\{x_s \\, : \\, BMU1(x_s) = i \\text{ and } BMU2(x_s) = j \\} \\] where BMU1 and BMU2 are the first and second-ranking BMU, respectively. Analysis of the CADJ values reveals the “seams” (disconnects) of a learned manifold, which are helpful for discerning cluster structure. While CADJ can technically be computed using the codebook of any vector quantizer as vertices, the SOM’s output space \\(\\mathcal{L}\\) provides prime real estate on which to view and analyze this graph, regardless of data dimension \\(d\\). The CONNvis visiualization [3], derived from the symmetrized CONNectivity graph, \\[CONN = CADJ + CADJ^T,\\] has been used repeatedly to elicit cluster structure from complex, high-dimensional manifolds with no restriction on cluster size (number of data members), shape (e.g., Gaussianinity), or dimension (low-rank embeddings). These settings are precisely where performance of other clustering routines (e.g., hierarchical methods, k-means, mixture models) deterioriate. Despite the demonstrated benefits and sophistication of the combination of CSOM manifold learning and subsequent CADJ-based cluster extraction (CITE), neither of these tools are currently publicly available to the machine learning community. SOMDisco rectifies this omission. References "],
["installation.html", "Chapter 2 Installation", " Chapter 2 Installation SOMDisco is available for Unix-like systems from github (not CRAN), installable via devtools::install_github(&quot;somdisco/SOMDisco&quot;) Windows users can clone the repository and modify their Makevars files accordingly. The DESCRIPTION file in the package source lists the following 3rd-party R package dependencies: Imports: Rcpp (&gt;= 1.0.1), stringr, igraph, dplyr, Rdpack LinkingTo: Rcpp, RcppArmadillo, RcppParallel SOM training is made efficient through use of a templated C++ CSOM class. For those who may desire custom or expanded functionality, the class template is implemented in header-only fashion in the inst/include directoy of the package source. "],
["shgr-example-data.html", "Chapter 3 SHGR Example Data", " Chapter 3 SHGR Example Data SOMDisco is packaged alongside an example dataset to showcase its training and analysis capabilities. The SHGRWalk Data (SHGR, for short) are 128 x 128 pixel, 100-dimensional Synthetic Hyperspectral data cubes based on a Gaussian Random Walk across the spectral “channels”. More information about the data generation and structure of the “20-class, cov500-4000” data case included with SOMDisco can be found at the SHGRWalk homepage. Briefly, library(SOMDisco) loads a list named SHGR into the user’s R environment with components: X the SHGR data, as a data matrix. nrows = 128 x 128 = 16,384 (number of image pixels), ncol = 100 (spectral dimension). Each row contains one spectral “signature” from each pixel of the SHGRWalk cube. label a character vector (length = 16,384) containing labels (“A” - “T”) for each pixel of the cube. The order corresponds to the row ordering of X. ctab a data frame with columns label and color defining the mapping between unique label names (“A”,“B”,…“T”) and their associated identifying color pxl.coords a 2-column matrix (nrows = 16,384) giving the spatial pixel coordinates ( in (row,col) format) for each spectra in the rows of X identifier a string identifying the specific SHGRWalk cube included with SOMDisco (there are others, as explained SHGRWalk homepage). The individual spectra of the pixels comprising each of the 20 unique classes, plotted across the 100 channels, is below: We will utilize the SHGR data in the training and visualization showcases which follow. "],
["som-initialization.html", "Chapter 4 SOM Initialization 4.1 Lattice Quantitites 4.2 Network Scaling 4.3 Prototype Initialization 4.4 Win Frequency Initialization 4.5 Learning Rate Initialization", " Chapter 4 SOM Initialization SOMDisco is built around the internal C++ class SOMOBJ, which is a header-only class (found in the /inst/include/ source package) envisioned to be entirely portable for future integration with other SOM-related projects. This section will demonstrate how to interact with the class fields and methods to achieve SOM training (a complete list of the fields and methods of this class exposed to the user by the SOMDisco package can be found with ?SOM). All methods are documented in the usual manner, with help functionality available via ?&lt;method_name&gt;. To get started with SOM training we must instantiate an empty SOM object, then initialize it with our training data and desired lattice size and topology via the method initialize_SOM. library(SOMDisco) mysom = SOM$new() mysom$initialize_SOM(SHGR$X, 20, 20, &quot;hex&quot;) ## Setting lattice quantities ## ++ calculating lattice (x,y) coordinates ... done ## ++ calculating neuron lattice adjacencies ... done ## ++ calculating geodesic lattice distances between neurons ... done ## ++ assigning geodesic lattice distances to distlist ... done ## ++ calculating lattice tile vertices ... done ## ---------------------------------------------------------------- ## Setting training data ## ---------------------------------------------------------------- ## Setting network ranges ## ++ external = [-115.19, 1347.57] ## ++ internal = [0.00, 1.00] ## ++ change defaults via $set_netrng ## ---------------------------------------------------------------- ## Initializing prototypes to random uniform ## ++ to set a particular random seed call set.seed() and then $set_W_runif() ## ++ to set to specific values call $set_W() ## ---------------------------------------------------------------- ## Initializing prototype win frequencies to equiprobable ## ++ to set to specific values call $set_p(values) ## ---------------------------------------------------------------- ## Setting default learning rates ## Storing the annealing schedule as: ## t alpha beta gamma sigma ## 16384 0.50 0.05 0.01 3 ## 81920 0.25 0.03 0.00 2 ## 163840 0.10 0.01 0.00 1 ## 409600 0.05 0.01 0.00 1 ## 1638400 0.01 0.00 0.00 1 ## ++ change via $get_LRAS and $set_LRAS ## ---------------------------------------------------------------- The arguments to initialize_SOM are, in order: the matrix with training data in rows, the desired som lattice width and height (number of neurons along each dimension) and the desired lattice type, which can be either &quot;hex&quot; for hexagonal lattices or &quot;grid&quot; for rectangular lattices. Internally, initialize_SOM calls several other methods to set required (and internally calculated) quantities related to the SOM lattice, and default parameters for network scaling, prototype initialization, and learning rate annealing. The sample size nX and data dimension d are also stored during this call. 4.1 Lattice Quantitites Lattice quantities are fields that generally begin with nu_* (by our convention, lattice neurons are represented by lower Greek \\(\\nu\\)). These fields can be examined via the $ operator of of the instantiated SOM object: # The lattice width, height, and type, respectively mysom$som_x ## [1] 20 mysom$som_y ## [1] 20 mysom$lattice_type ## [1] &quot;hex&quot; # The total number of neurons / prototypes in the SOM. # For hexagonal lattices, this number will be &gt; som_x*som_y mysom$nW ## [1] 410 # The (x,y) coordinates of the neurons on the SOM lattice str(mysom$nu_xy) ## num [1:410, 1:2] 1 2 3 4 5 6 7 8 9 10 ... # The (row,col) coordinates of the neurons on the SOM lattice. # These differ from their (x,y) coordinates if lattice_type = &#39;hex&#39; str(mysom$nu_ij) ## num [1:410, 1:2] 1 1 1 1 1 1 1 1 1 1 ... # A binary adjacency matrix representing the topology of neurons on the lattice str(mysom$nu_ADJ) ## num [1:410, 1:410] 1 1 0 0 0 0 0 0 0 0 ... # The (x,y) coordinates of the vertices of each lattice tile are stored # in a 3d cube whose slices follow neuron ordering. str(mysom$nu_verts) ## num [1:6, 1:2, 1:410] 1.5 1 0.5 0.5 1 ... # e.g., the vertices of the lattice tile centered at the first neuron are: mysom$nu_verts[,,1] ## [,1] [,2] ## [1,] 1.5 1.2886751 ## [2,] 1.0 1.5773503 ## [3,] 0.5 1.2886751 ## [4,] 0.5 0.7113249 ## [5,] 1.0 0.4226497 ## [6,] 1.5 0.7113249 # A list giving, for all neurons, the indices of others neurons that are # within a certain lattice geodesic distance: length(mysom$nu_nhblist) ## [1] 410 # The vector nu_nhblist[[i]][[j]] contains indices of neurons that are # within geodesic lattice distace j-1 of neuron i. # ex., these neurons are within lattice distance = 1 of the first lattice neuron: mysom$nu_nhblist[[1]][[2]] ## [1] 2 21 22 The above quantities are set by the method set_lattice, but this should rarely need to be called on its own (it is called internally by initialize_SOM). All containers storing neuron and prototype quantities are ordered such that the first element (or row, or slice, as applicable) corresponds to the neuron at the bottom left of the SOM lattice. Ordering proceeds across the rows of the lattice so that the last element (row, slice) corresponds to the neuron at the top right of the SOM. This is known as neuron ordering 4.2 Network Scaling The external network range is the apparent range of the training data. Internally, the SOM prototypes are stored in an internal&quot; network range, which is necessary to stabilize and accelerate the training process. Both BMU selection and prototype updates are performed in the internal network range. During training, data are mapped (linearly) from the external to internal range for presentation to the network. initialize_SOM calls the method set_netrng internally, which sets the min/max of both the external and internal network ranges needed for this mapping to their defaults (which is the apparent dimension-wise range of the training data for the former, and [0,1] for the latter): # The default values stored str(mysom$netrng_ext_min) ## num [1, 1:100] -115 -115 -115 -115 -115 ... str(mysom$netrng_ext_max) ## num [1, 1:100] 1348 1348 1348 1348 1348 ... mysom$netrng_int_min ## [1] 0 mysom$netrng_int_max ## [1] 1 The default network scalings can be changed manually by calling set_netrng at any point prior to training. See its help for more information. Once the network ranges are set, data can be mapped from external to internal network ranges (or vice-versa) via the methods map_to_netrng and map_from_netrng, both of which are parallelized. For example, the entire training set can be mapped to the internal network range via: range(SHGR$X) ## [1] -115.1884 1347.5741 mapped_training_data = mysom$map_to_netrng(SHGR$X) range(mapped_training_data) ## [1] 0 1 Commonly, the learned SOM prototypes would need to be extracted from the SOM object and returned to external range for direct comparison with the data. This can be achived by: range(mysom$W) ## [1] 0.5000002 0.5999968 mapped_prototypes = mysom$map_from_netrng(mysom$W) range(mapped_prototypes) ## [1] 616.1932 762.4644 4.3 Prototype Initialization The SOM prototype weights are stored in the rows of the nW x d prototype matrix W: str(mysom$W) ## num [1:410, 1:100] 0.595 0.57 0.589 0.518 0.563 ... initialize_SOM calls the method set_W_runif automatically, which initializes the prototypes to random uniform values over the middle 10% of the internal network range. If random seeding of these values is desired for reproducibility, this method should be re-invoked immediately after a call to R’s set.seed: set.seed(123) mysom$set_W_runif() Custom prototype initializations are supported by the general method set_W, which allows the user to set a desired prototype weight matrix (which must have conforming dimensions: nrows = nW and ncols = d) Here, we set them to random standard normals for demo purposes: rnorm_protos = matrix(rnorm(mysom$nW*mysom$d), nrow=mysom$nW, ncol=mysom$d) mysom$set_W(rnorm_protos) ## Warning in mysom$set_W(rnorm_protos): Input is outside netrng_int. Likely need ## to scale to internal network range, then call $set_W() A warning is issued if the prototype matrix contains values outside the internal network range. Custom initializations will likely need to be mapped to internal range prior to setting: rnorm_protos = mysom$map_to_netrng(rnorm_protos) mysom$set_W(rnorm_protos) 4.4 Win Frequency Initialization In addition to the prototypes themselves, DeSieno’s CSOM algorithm introduces another learned quantity to facilitate a maximum-entropy SOM mapping: the prototype win frequencies, which are stored in the SOM object in the vector p. The p control the CSOM prototype bias, which affects the competitive stage of BMU selection during SOM training. initialize_SOM automatically calls the method set_p_equal, which sets all prototype win frequences to the value 1 / nW. If desired, other initializations can be achieved via the method set_p, which takes a user-specified vector of win frequencies: new_p = runif(mysom$nW) mysom$set_p(new_p) 4.5 Learning Rate Initialization All CSOM learning rates should be annealed over time. Annealing is controlled by the internally stored data frame LRAS (or Learning Rate Annealing Schedule), which has columns t (setting the cumulative number of training steps over which the given rates are valid), alpha (controlling the strength of the prototype updates), beta (controlling the strength of win frequncy updates), gamma (controlling the influence of the win frequences to the CSOM bias), and sigma (controlling the lattice neighborhood size over which prototype updates occur). initialize_SOM calls the stand-alone function default_LRAS by default, which populates an annealing schedule based solely on the training sample size: default_LRAS(nX = mysom$nX) ## t alpha beta gamma sigma ## 1 16384 0.50 0.050 0.0050 3 ## 2 81920 0.25 0.025 0.0025 2 ## 3 163840 0.10 0.010 0.0010 1 ## 4 409600 0.05 0.005 0.0005 1 ## 5 1638400 0.01 0.001 0.0001 1 Users can check the current annealing schedule of the SOM object via the method get_LRAS and set it to new values with set_LRAS, which takes a data frame of the above form as input. For example, the below will increase the alpha values for all training steps by 10%: new_LRAS = mysom$get_LRAS() new_LRAS$alpha = 1.10 * new_LRAS$alpha mysom$set_LRAS(new_LRAS) ## Storing the annealing schedule as: ## t alpha beta gamma sigma ## 16384 0.55 0.05 0.01 3 ## 81920 0.28 0.03 0.00 2 ## 163840 0.11 0.01 0.00 1 ## 409600 0.06 0.01 0.00 1 ## 1638400 0.01 0.00 0.00 1 At every training step the currently applicable learning rates (based on the SOM’s age and the LRAS schedule) are extracted from the LRAS data frame and stored in the following fields by the method update_learning_rates: mysom$alpha ## [1] 0.55 mysom$beta ## [1] 0.05 mysom$gamma ## [1] 0.005 mysom$sigma ## [1] 3 The parameter sigma controls the size of the (lattice) neighborhood (of the prevailing BMU) in which prototypes are updated during each training step. The strength of neighbor updates decreases with their lattice distance from the BMU; the prototypes of any neurons which are greater than sigma away from the BMU will not be updated. The strength of updates to prototype whose neurons are within a sigma lattice radius of the BMU decreases exponentially with this lattice distance. This factor is stored in the SOM object as the parameter eta, which is a vector of length sigma + 1. The vector element eta[r] stores a multiplicative coefficient applied to the updates of those prototypes which are distance r-1 from the BMU. mysom$eta ## [,1] ## [1,] 1.0000000 ## [2,] 1.0000000 ## [3,] 0.5994843 ## [4,] 0.3593814 eta is re-calculated any time the effective sigma value is changed (annealed) during training. To enforce the organization of the SOM, eta always = 1 for the BMU and its immediate lattice neighbors. eta is calculated via the method calc_eta, which should rarely need to be called on its own. Of note, Kohonen’s original prototype algorithm (i.e, without DeSieno’s Conscience modification) can be achieved by setting beta and gamma to 0 and annealing a larger sigma radius over time. "],
["som-learning.html", "Chapter 5 SOM Learning 5.1 Network Training 5.2 Network Recall 5.3 Recalling Labeled Data 5.4 Monitoring Learning 5.5 Parallel Processing 5.6 Saving &amp; Loading", " Chapter 5 SOM Learning 5.1 Network Training While several methods exposed to the user participate in SOM training, the main method (and, typically, the only method needing to be called) for SOM training, is train_SOM. As the SOM class was intetionally designed to not store the training data (to avoid duplicate memory occupation of larger training sets), the matrix of training data must be passed to this method. It should be the same data matrix supplied when calling initialize_SOM (internal checks will verify this, and return a runtime error if deviations are deteced). The following command performs 100,000 training steps on an SOM object previously initialized to learn the example SHGR data. Status information is automatically printed to the R console. mysom$train_SOM(100000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 0steps ## 1 2 3 4 5 6 7 8 ## 9 10 ## End Training (current age = 100000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done mysom$age ## [1] 1e+05 Internally, train_SOM calls the methods update_W and update_p, which perform updates to the prototype matrix W and the CSOM win frequencies p at each training step. These methods are exposed to the user, but should not be called separately independently. The age field stores the number of training steps that have been performed during the life of the instantiated SOM object. Additional training can be performed at any time by re-invoking the train_SOM method. Here, we train the above SOM for another 25,000 steps: mysom$train_SOM(25000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 0steps ## 1 2 ## End Training (current age = 125000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done mysom$age ## [1] 125000 Training should proceed until the user is satisfied with the SOM’s prototype development and neuron lattice organization. The learned SOM prototype vectors can be extracted from the SOM object via the W field, and should be mapped from internal to external network range if direct comparison to their representative data vectors is desired. Here, we extract the prototypes after the cumulative 125,000 learning steps performed above: W125k = mysom$W # Return to external network (i.e, data) range W125k = mysom$map_from_netrng(mysom$W) Standard online (i.e, not batch) SOM training requires random-order presentation of training data to the network at each training step. A user-specified presentation order for training can be achieved by calling the method set_train_order just prior to invoking train_SOM. set_train_order takes a vector of data indices (of length nsteps), which sets the order of the data picked for presentation to the network for the next nsteps training steps. Its input should be 1-based row indices of the training data matrix. Here, we train the above SOM for an additional 10,000 steps where data presentation to the network follows a spefic order: train_order = 1:10000 mysom$set_train_order(train_order) mysom$train_SOM(10000, SHGR$X) ## User-specified training order detected. nsteps reset to 10000 ## SOM Training: ## ++ Reporting every 10000, monitoring every 0steps ## 1 ## End Training (current age = 135000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done mysom$age ## [1] 135000 If a specific training order is supplied the number of steps argument to train_SOM is ignored, and the SOM is trained for nsteps = length(train_order). Setting a training order is uncommon, useful mostly for experimentation and testing. More commonly a user may wish to retain random presentation of data to the network while also enforcing reproducibility. This can be achieved by calling R’s set.seed function just prior to train_SOM: set.seed(1001) mysom$train_SOM(10000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 0steps ## 1 ## End Training (current age = 145000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done mysom$age ## [1] 145000 5.2 Network Recall After training, the entire training set should be recalled through the SOM, via the method recall_SOM, which produes additional useful products of SOM learning. A call to train_SOM automatically ends with a recall, but the recall function is exposed to the user for completeness. Again, it must be given the entire training data matrix (which should match that supplied during initialize_SOM and train_SOM): mysom$recall_SOM(SHGR$X) ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done A recall populates several fields related to the SOM mapping, as described below. 5.2.1 BMU - SOM Forwad Map The BMU – or Best Matching Unit – of a data vector \\(x_s\\) is the learned prototype index \\(j^*\\) satisfying \\[ j^* = \\arg\\min_{j} \\, d_E(w_j, x_s)^2 - b_j\\] where \\(d_E(\\cdot,\\cdot)^2\\) is squared Euclidean distance between its vector arguments and \\(b_j\\) is the learned per-prototype CSOM bias. For a variety of reasons it is useful to calculate and store at least the first and second BMUs (denoted BMU1 and BMU2) for each datum. In general, \\(\\text{BMU}[k](x_s)\\) = the k-th Best Matching Unit (prototype) for the datum indexed by \\(s\\). The field nBMU controls the number of BMUs computed and stored during recall, and can be set by the method set_nBMU prior to recall. A default nBMU = 2 is set during a call to initialize_SOM. Here, we change the number of BMUs to 3, and repeat the recall: mysom$set_nBMU(3) mysom$recall_SOM(SHGR$X) ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done Note that nBMU must be \\(\\geq 2\\). The BMU information is stored in the field BMU, which is a matrix (nrows = nX, ncols = nBMU) whose (i,j) element gives the j-th BMU index (i.e., the j-th closest prototype) for the datum in row i of the training data matrix. This information can be access via head(mysom$BMU) ## [,1] [,2] [,3] ## [1,] 8 9 10 ## [2,] 10 31 51 ## [3,] 10 9 31 ## [4,] 51 31 72 ## [5,] 9 8 10 ## [6,] 72 52 51 BMU stores the forward SOM mapping; that is, the mapping produced from the network input space (i.e., data space, or \\(\\mathbb{R}^d\\)), to the network output space (i.e, the neurons on the SOM lattice). 5.2.2 RF_members - SOM Reverse Map The reverse SOM mapping is also stored during recall_SOM in the field RF_members, where the prefix “RF” stands for Receptive Field . This field identifies the mapping from SOM output space to input space, represented by a list of length = nW whose i-th entry is a vector of the 1-based data indices (rows of the training data matrix) for whom prototype i is BMU1. For example, the data indices in the Receptive Field of prototype index 10 can be accessed via mysom$RF_members[[10]] ## [1] 2 3 11 150 265 269 278 283 386 391 397 403 411 414 514 ## [16] 518 523 531 542 644 647 649 650 658 668 670 671 770 776 792 ## [31] 797 900 902 915 1157 1162 1163 1170 1171 1174 1175 1177 1178 1281 1295 ## [46] 1303 1412 1425 1432 1545 1550 1551 1553 1568 1665 1666 1675 1684 1690 1692 ## [61] 1696 1793 1800 1813 1815 1818 1929 1931 1935 1937 1938 1941 1947 2063 2185 ## [76] 2190 2196 2308 2314 2322 2323 2433 2435 2444 2445 2570 2574 2592 2690 2693 ## [91] 2699 2826 2952 2961 2964 2966 2970 2976 3074 3076 3077 3079 3098 3099 3205 ## [106] 3225 3228 3232 3345 3347 3349 3458 3472 3475 3483 3484 3589 3599 3601 3603 ## [121] 3615 3719 3725 3727 3733 3858 3860 3866 3982 3983 3996 While CSOM attempts an equiprobable quantization of the entire training set by the prototypes, some prototypes will have empty Receptive Fields (meaning they do not quantize any data). If prototype \\(j\\)’s Receptive Field is empty, then RF_members[[j]] is an empty vector (of length = 0). 5.2.3 RF_size The size of a prototype’s receptive field is the number of datum it quantizes (the size of RF_members[[j]]). This information is stored in the field RF_size, accessible via head(mysom$RF_size, n=10) ## [,1] ## [1,] 83 ## [2,] 50 ## [3,] 81 ## [4,] 0 ## [5,] 30 ## [6,] 141 ## [7,] 0 ## [8,] 62 ## [9,] 59 ## [10,] 131 Note that sum(RF_size) = nX (all data are represented by some prototype, hence the sum of the sizes of all prototype’s Receptive Fields must equal the training sample size). RF_size is useful both in analysis and visualization, as large deviations in Receptive Field size between neighboring lattice neurons can indicate cluster boundaries or pattern transitions on the SOM. 5.2.4 Quantization Error The distance between a datum \\(x_s\\) and its BMU \\(j^*\\) represents the Quantization Error induced by quantizing \\(x_s\\) by \\(w_{j^*}\\). These (squared) distances are computed during BMU selection and stored in the field SQE, which is a vector (length = nX) of the Squared Quantization Error arising from the SOM mapping, accessible via str(mysom$SQE) ## num [1:16384, 1] 0.0927 0.0843 0.0604 0.1515 0.1142 ... 5.2.5 Entropy The normalized entropy of the forward SOM mapping is stored in the field Entropy. The discrete representation of the manifold from which the training data was drawn drives this entropy calculation, defined as \\[ \\text{Entropy} = \\frac{1}{\\log_2(nW)} \\sum\\limits_{j=1}^{nW} \\frac{\\text{RF\\_size}_j}{nX} \\times \\log_2\\left( \\frac{\\text{RF\\_size}_j}{nX} \\right)\\] The above is normalized so that \\(0 \\leq \\text{Entropy} \\leq 1\\), and \\(\\text{Entropy} = 1\\) means all prototypes quantize the same number of training data vectors. The normalized entropy of the current SOM mapping is accessible via mysom$Entropy ## [1] 0.9461503 5.2.6 U-Matrix Fences The U-Matrix [5] and mU-Matrix [6] visualizations require computing distances between all prototypes whose neurons are lattice-adjacenct. This is done automatically during recall_SOM using the learned prototype vectors in W and the lattice adjacency matrix nu_ADJ. The results are stored in the field fences, which is a data frame whose rows represent individual fence information of the form head(mysom$fences) ## i j x0 y0 x1 y1 value ## 1 1 2 1.5 0.7113249 1.5 1.288675 0.011281060 ## 2 2 3 2.5 0.7113249 2.5 1.288675 0.011196895 ## 3 3 4 3.5 0.7113249 3.5 1.288675 2.898032735 ## 4 4 5 4.5 0.7113249 4.5 1.288675 0.432551597 ## 5 5 6 5.5 0.7113249 5.5 1.288675 0.003857276 ## 6 6 7 6.5 0.7113249 6.5 1.288675 2.707473984 The columns i and j give the two neurons (their indices) which are separated by the fence in each row; the columns x0, y0, x1, y1 give the \\((x,y)\\) coordinates of the endpoints of each fence (as visualized on the lattice); the value column stores the squared Euclidean distance between prototypes \\(W_i\\) and \\(W_j\\). This data frame can be populated by a separate call to method set_lattice_fences but this is generally unnecessary as it is done automatically in recall_SOM. 5.2.7 CADJ &amp; CONN The CADJ matrix [3] is a weighted adjacency matrix of SOM prototypes (neurons) whose \\((i,j)\\) elements are defined by \\[ \\text{CADJ}_{ij} = \\#\\{x_s \\, : \\, BMU1(x_s)=i \\text{ and } BMU2(x_s)=j \\}. \\] Thus, the matrix entries count the number of data vectors for whom prototype \\(i\\) is first-BMU and \\(j\\) is second-BMU. This matrix, when visualized on the SOM lattice, is useful for inferring the topological (dis-)connectivities of the manifold underlying the data \\(X\\) in \\(\\mathbb{R}^d\\). It is accessible via the CADJ field of a recalled SOM object: str(mysom$CADJ) ## num [1:410, 1:410] 0 13 0 0 0 0 0 0 0 0 ... Its symmetrized version, called CONN, is also accessible. Since \\(CONN = CADJ + CADJ^T\\) it is never stored in the SOM object, merely recreated upon demand. As such, CONN is accessible by calling the CONN method (not field): str(mysom$CONN()) ## num [1:410, 1:410] 0 35 0 0 0 0 0 0 0 0 ... 5.3 Recalling Labeled Data If the training data possesses associated discrete labels (such as cluster or class membership) it is often helpful to propagate these labels through the forward SOM mapping (which is known as “labeling the prototypes”) to assess the organization of the lattice, determine the quality of the labels, or both. If the labels are meaningful (such that they define genuinely distinct data classes or clusters), and if SOM learning is adequate (i.e., performed correctly over enough training steps), then one expects an organization of prototype labels on the lattice. Propagation of data labels through the SOM is achieved by the method set_RF_label, which takes a vector of character labels, with one entry for every row of the training data \\(X\\). Using the labels of the example SHGR data stored in SHGR$label, label propagation is performed via mysom$set_RF_label(SHGR$label) The above method sets two different fields of the SOM object: RF_label_dist is a list (length = nW) of named integer vectors representing the distribution (contingency count) of the labels mapped to each prototype’s Receptive Field. The names attribute of the vector stores the label whereas the vector elements store the associated count. Obviously, the sum of this vector equals the corresponding RF_size. RF_label is a character vector (length = nW) of the “winning label” in each RF, which is the plurality winning label of each RF’s distribution (found in RF_label_dist). The plurality winner for a RF is the label with highest contingency count. For example, the table of label counts and the plurality label of the first prototype in the SHGR SOM are: mysom$RF_label_dist[[1]] ## J ## 83 mysom$RF_label[1] ## [1] &quot;J&quot; These fields are primarily useful for label visualization, discussed in Section 6? The SOM object must be trained (and recalled) prior to calling set_RF_label. 5.4 Monitoring Learning While there is no universal criteria to determine the number of training steps required for an SOM to properly learn and represent arbitrary manifolds there are a few rules of thumb which help assess the suitability of such learning. During training the SOM undergoes two phases: the initial organization phase aligns the neurons on the lattice such that the resulting mapping is topology preserving (which usually occurs relatively quickly) while the convergence phase further develops the prototypes to best represent the input data (which can take quite some time, depending on the learning parameters used). As a vector quantizer, the SOM should be trained until it is stable and represents the input data reasonably well. To assess these criteria snapshots of the SOM mapping can be taken at various points in (training) time and compared to each other to determine a suitable point at which to terminate training. The first criterion above is represented by the QSI – or Quantization Stability Index – which, at any snapshot time step \\(\\tau\\), reports the proportion of data vectors whose BMUs have changed since the previous snapshot at time \\(\\tau - 1\\). That is, \\[ QSI(\\tau) = \\frac{1}{nX} \\sum\\limits_{s=1}^{nX} \\#\\{ BMU_{\\tau-1}(x_s) \\, \\neq \\, BMU_{\\tau}(x_s) \\}\\] As proper learning proceeds over time we expect \\(QSI(\\tau) \\to 0\\). The second criterion above is captured by the overall RMSQE – or Root Mean Square Quantization Error – of the mapping, which is the root average of the SQE of each data vector (as explained in Section 5.2). At snapshot time \\(\\tau\\) this is \\[ RMSQE(\\tau) = \\sqrt{ \\frac{1}{nX} \\sum\\limits_{s=1}^{nX} d_E^2(x_s, w_{j^*_{\\tau}}) } \\] where \\(d_E^2(\\cdot,\\cdot)\\) is squared Euclidean distance and the notation \\(w_{j^*_{\\tau}}\\) denotes the prevailing prototype BMU of data vector \\(x_s\\) at snapshot time \\(\\tau\\).Conscience-SOM learning provides a third suitable metric to assess the quality of SOM training. Designed to produce maximum-entropy SOM mappings, a CSOM should be trained until its Entropy (section 5.2) is maximal, or at least has stabilized at some near-maximal plateau. Monitoring of these quantities can be activated prior to SOM training via the method set_monitoring_freq, which takes as argument the incremental number of steps between monitoring snapshots and stores this argument in the field mtr_freq. By default, initialize_SOM sets mtr_freq = 0 which means no monitoring is performed. Here, we reset the SHGR SOM and repeat training for 100,000 steps with monitoring activated at every 10,000 steps: # Initialize the SOM mysom = SOM$new() mysom$initialize_SOM(SHGR$X, 20, 20, &quot;hex&quot;) ## Setting lattice quantities ## ++ calculating lattice (x,y) coordinates ... done ## ++ calculating neuron lattice adjacencies ... done ## ++ calculating geodesic lattice distances between neurons ... done ## ++ assigning geodesic lattice distances to distlist ... done ## ++ calculating lattice tile vertices ... done ## ---------------------------------------------------------------- ## Setting training data ## ---------------------------------------------------------------- ## Setting network ranges ## ++ external = [-115.19, 1347.57] ## ++ internal = [0.00, 1.00] ## ++ change defaults via $set_netrng ## ---------------------------------------------------------------- ## Initializing prototypes to random uniform ## ++ to set a particular random seed call set.seed() and then $set_W_runif() ## ++ to set to specific values call $set_W() ## ---------------------------------------------------------------- ## Initializing prototype win frequencies to equiprobable ## ++ to set to specific values call $set_p(values) ## ---------------------------------------------------------------- ## Setting default learning rates ## Storing the annealing schedule as: ## t alpha beta gamma sigma ## 16384 0.50 0.05 0.01 3 ## 81920 0.25 0.03 0.00 2 ## 163840 0.10 0.01 0.00 1 ## 409600 0.05 0.01 0.00 1 ## 1638400 0.01 0.00 0.00 1 ## ++ change via $get_LRAS and $set_LRAS ## ---------------------------------------------------------------- # Set the desired snapshot window mysom$set_monitoring_freq(10000) mysom$mtr_freq ## [1] 10000 # Train mysom$train_SOM(100000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 10000steps ## 1 2 3 4 5 6 7 8 ## 9 10 ## End Training (current age = 100000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done The status message displayed during training will indicate the monitoring frequency. After training, the monitored quantities can be extracted via: # The training ages at which snapshots were taken head(mysom$mtr_age) ## [1] 10000 20000 30000 40000 50000 60000 # The QSI at each monitoring snapshot. # By convention, the first QSI is NA because it compares two snapshots head(mysom$mtr_QSI) ## [1] NA 0.9555664 0.9020996 0.9281616 0.9133301 0.9232788 # The RMSQE at each monitoring snapshot head(mysom$mtr_RMSQE) ## [1] 0.3364690 0.3166922 0.3168527 0.3184309 0.3154855 0.3167003 # The Entropies at each monitoring snapshot head(mysom$mtr_Entropy) ## [1] 0.8070174 0.8952083 0.8826630 0.8791978 0.8933543 0.8902079 These quantities can be visualized as time series for user analysis (see Section 6). Note: Most of the recall quantities described above will be computed at each monitoring snapshot. Care should be taken to set mtr_freq appropriately: it should be small enough to capture meaningful trends in the monitored quantities but also large enough to not incur substantial increase in training time, as the recall is a computationally expensive part of SOM learning. 5.5 Parallel Processing SOMDisco’s parallel implementation is supported by RcppParallel. By default, initialize_SOM activates parallel computation of all possible quantities involved in SOM training and recall. For online (sequential) SOM training, this includes BMU selection and prototype updates at each training step. Most recall quantities can be processed in parallel. The parallel field of an instantiated SOM object controls whether computation is performed in parallel. To disable parallel processing, the set_parallel method can be used to set parallel = FALSE: # View existint parallel status mysom$parallel ## [1] TRUE # To disable mysom$set_parallel(FALSE) # Check mysom$parallel ## [1] FALSE Machine specific threading can be controlled via RcppParallel functions. To control the number of threads made available for parallel processing, see ?RcppParallel::defaultNumThreads and ?RcppParallel::setThreadOptions. When SOMDisco is loaded (via library(SOMDisco)), the following command is issued to the current R environment, which reserves one thread from processing for other uses: RcppParallel::setThreadOptions(numThreads = RcppParallel::defaultNumThreads() - 1) 5.6 Saving &amp; Loading Trained SOM objects can be saved to disk for future access via the save method. This method requires a user-specified file name and must have extension .som. # Save method, filename must have .som extension mysom$save(&quot;mysom.som&quot;) While the file has extension .som it is saved in R’s binary .rds format as a list. R’s load command can be used to return this list to the user’s environment in a new R session, but the pointers to the C++ object are destroyed on exit of the first session. The load method will properly restore a trained and previously saved SOM object to memory. Once loaded all object methods are available. For example, we can load the previously saved SOM and continue training (with the same SHGR data): # Load the prevoiusly saved .som file mysom2 = SOMDisco::SOM$new() mysom2$load(&quot;mysom.som&quot;) ## ++ calculating lattice (x,y) coordinates ... done ## ++ calculating neuron lattice adjacencies ... done ## ++ calculating geodesic lattice distances between neurons ... done ## ++ assigning geodesic lattice distances to distlist ... done ## ++ calculating lattice tile vertices ... done ## Storing the annealing schedule as: ## t alpha beta gamma sigma ## 16384 0.50 0.05 0.01 3 ## 81920 0.25 0.03 0.00 2 ## 163840 0.10 0.01 0.00 1 ## 409600 0.05 0.01 0.00 1 ## 1638400 0.01 0.00 0.00 1 # Check that it is the same mysom2$age ## [1] 1e+05 # Re-train for another 10000 steps mysom2$train_SOM(10000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 10000steps ## 1 ## End Training (current age = 110000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done References "],
["secVis.html", "Chapter 6 A Complete Example with Visualizations", " Chapter 6 A Complete Example with Visualizations In this section we exercise a typical standard use of SOMDisco and demonstrate how the supporting SOM visualization functions can be used to aid inference and cluster extraction from a trained SOM. The SOM is unique among vector quantizers in that it possesses a predefined output topology (typically 2-d, as is our setting) which can be harnessed to compactly express the information learned by an SOM for easy inspection. For speed, all custom SOM visualizations are predicated on R’s base::plot functionality (vs., e.g., ggplot). Each of the visualization functions demonstrated in this section are intended to work together (via plot layering) to allow simultaneous representation of multiple learned SOM quantities. Many customization options exist for the visualization functions; their individual help menus explain this optionality. library(SOMDisco) library(TopoRNet) To begin, we will setup a new SOM object for SHGR training: # Initialize a 20x20 SOM with hexagonal lattice mysom = SOM$new() mysom$initialize_SOM(SHGR$X, 20, 20, &quot;hex&quot;) ## Setting lattice quantities ## ++ calculating lattice (x,y) coordinates ... done ## ++ calculating neuron lattice adjacencies ... done ## ++ calculating geodesic lattice distances between neurons ... done ## ++ assigning geodesic lattice distances to distlist ... done ## ++ calculating lattice tile vertices ... done ## ---------------------------------------------------------------- ## Setting training data ## ---------------------------------------------------------------- ## Setting network ranges ## ++ external = [-115.19, 1347.57] ## ++ internal = [0.00, 1.00] ## ++ change defaults via $set_netrng ## ---------------------------------------------------------------- ## Initializing prototypes to random uniform ## ++ to set a particular random seed call set.seed() and then $set_W_runif() ## ++ to set to specific values call $set_W() ## ---------------------------------------------------------------- ## Initializing prototype win frequencies to equiprobable ## ++ to set to specific values call $set_p(values) ## ---------------------------------------------------------------- ## Setting default learning rates ## Storing the annealing schedule as: ## t alpha beta gamma sigma ## 16384 0.50 0.05 0.01 3 ## 81920 0.25 0.03 0.00 2 ## 163840 0.10 0.01 0.00 1 ## 409600 0.05 0.01 0.00 1 ## 1638400 0.01 0.00 0.00 1 ## ++ change via $get_LRAS and $set_LRAS ## ---------------------------------------------------------------- # For reproducibility, re-initialize the prototypes with a given seed set.seed(123) mysom$set_W_runif() The lattice can be immediately visualized after initialization. # Setup an SOM plot window vis_som_setup(SOM=mysom, lattice_coords = T) # Add the lattice tiles as polygons vis_som_tiles(SOM=mysom, add=T) # Add the neurons as points vis_som_neurons(SOM = mysom, add = T) The SOM visualizations are intended to be setup via a call to vis_som_setup then further augmented: in this case with the lattice tiles (whose shape is dependent upon lattice_type: squares for “grid” types, and hexagons for “hex” types) and neurons. Most visualization functions have an add argument, which if set to FALSE will call vis_som_setup internally (thus bypassing the need to explicitly layer the functions with individual calls). We set the SOM to monitor every 10,000 steps and train for 500,000 steps: mysom$set_monitoring_freq(10000) mysom$train_SOM(500000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 10000steps ## 1 2 3 4 5 6 7 8 ## 9 10 11 12 13 14 15 16 ## 17 18 19 20 21 22 23 24 ## 25 26 27 28 29 30 31 32 ## 33 34 35 36 37 38 39 40 ## 41 42 43 44 45 46 47 48 ## 49 50 ## End Training (current age = 500000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done If monitoring is activated, the quantities discussed in Section WHERE? can be visualized after training: vis_som_training(SOM = mysom, vis.SOM = T, vis.mtr = T) The top panel displays a heatmap of the RF_size of each prototype’s Receptive Field. White tiles correspond to empty Receptive Fields, black to low and light blue to high RF_size; combined, this is the mU-Matrix visualization of [6] Clusters in these data are readily visible, separated by areas of “dead” neurons, which do not represent any training data. The lower panel plots the RMSQE, QSI and Entropy (recorded every 10k training steps) vs training age. While RMSQE and Entropy have been somewhat stable over most of the current age of the SOM QSI has not, indicating further training may be beneficial. We continue training another 500k steps and re-create the monitoring visualization: # Train another 500k steps and view the resulting age mysom$train_SOM(500000, SHGR$X) ## SOM Training: ## ++ Reporting every 10000, monitoring every 10000steps ## 1 2 3 4 5 6 7 8 ## 9 10 11 12 13 14 15 16 ## 17 18 19 20 21 22 23 24 ## 25 26 27 28 29 30 31 32 ## 33 34 35 36 37 38 39 40 ## 41 42 43 44 45 46 47 48 ## 49 50 ## End Training (current age = 1000000) ## ---------------------------------------------------------------- ## SOM Recall: ## ++ finding BMUs of data ... done ## ++ building CADJ matrix ... done ## ++ setting RF_size ... done ## ++ calculating SOM Entropy ... done ## ++ populating RF_members ... done ## ++ setting lattice fences ... done mysom$age ## [1] 1e+06 # Visualize Training vis_som_training(SOM = mysom, vis.SOM = T, vis.mtr = T) After training we can assess the organization of the lattice by plotting the 100-d SOM prototype vectors in their respective lattice tiles: # Setup the lattice tiles vis_som_setup(SOM = mysom, lattice_coords = T) vis_som_tiles(SOM = mysom, add = T) # Add the prototype vectors in lattice situ vis_som_prototypes(SOM=mysom, add=T) Clusters of similar prototype shapes are readily apparent, confirming the organization of this SOM. We can explore these apparent clusters with the CONNvis visualization which requires the TopoRNet package to be installed as well. Using the CONN matrix just learned we define a TRN object and compute the CONNvis statistics mytrn = TRN$new() mytrn$set_input_topo(mysom$W, mysom$CONN()) ## Setting Input Space Topos ## ++ storing vertices ... done ## ++ storing edge list ... done mytrn$set_output_topo(mysom$nu_xy, mysom$nu_ADJ) ## Setting Output Space Topos ## ++ storing vertices ... done ## ++ storing edge list ... done ## ++ computing folding lengths of forward network mapping ... done ## ++ computing folding lengths of backward network mapping ... ## Warning in mytrn$set_output_topo(mysom$nu_xy, mysom$nu_ADJ): NAs introduced by ## coercion to integer range ## done ## ++ computing topology preserving radius ... done mytrn$set_CONNvis() ## CONNvis parameterization:++ computing local ranks ... done ## ++ computing local rank stats ... done ## ++ computing global ranks ... done ## ++ computing global rank stats ... done ## ++ computing output topology length stats ... done which are passed, along with our trained SOM, to the function vis_som_CONNvis: vis_som_CONNvis(SOM = mysom, TRN = mytrn, add = F) CONNvis ranks each CONN edge by local and global topological importance, indicated by color (red, blue, green, yellow, grayscale) and line width. As recognized above, many clusters are readily apparent on the SOM with the mU-Matrix visualization alone. CONNvis has further revealed evidence of separation of large clusters in the middle-left and middel-right of the SOM. Since these training data are labeled we can project the data labels through the learned SOM mapping to expand the CONNvis findings. mysom$set_RF_label(SHGR$label) Visualizations involving labels require an additional data frame known as a color table or ctab to function which controls the coloring of each character label found in the dataset. The ctab should be a data frame with columns label (listing the distinct character labels) and color giving a hex color code (or R color name) corresponding to each label. The SHGR data is packaged with the following color table: head(SHGR$ctab) ## label color ## 1 A #0F52BA ## 2 B #FF7417 ## 3 C #4CBB17 ## 4 D #ED2939 ## 5 E #B660CD ## 6 F #FFF200 which can be visualized via vis_ctab(ctab=SHGR$ctab) With a color table set, the label distribution in each prototype’s Receptive Field can be visualized on the lattice as a proportional pie chart in each lattice tile: # Setup the lattice tiles vis_som_setup(SOM = mysom, lattice_coords = T) vis_som_tiles(SOM = mysom, add = T) # View the mapped label distribution vis_som_labeldist(SOM=mysom, ctab=SHGR$ctab, add=T) The neighborhoods on the SOM appear to be meaningful, with the majority of prototype Receptive Fields containing data of the same label. There is some visible mixing of labels in the RFs of “boundary prototypes” which sit between neighboring clusters, which is to be expected for data of any complexity. Often, an analyst may only care about the plurality winning label within each Receptive Field, which can be viewed by “painting” the SOM tiles with their corresponding label color, and annotating each tile with its label: # Setup the lattice tiles, colored by prototype label vis_som_setup(SOM = mysom, lattice_coords = T) vis_som_label(SOM = mysom, ctab = SHGR$ctab, add = T) # Annotate the tiles with the label vis_som_annotate(SOM = mysom, add = T, text = mysom$RF_label) The propagated labels display roughly the same prototype clustering as suggested by the CONNvis above, confirming both our suggested clustering and the quality of the SHGR labeling. References "],
["references.html", "References", " References "]
]
